{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5a (German tank problem).** During WW2, the Allies discovered that German tanks carry consecutive serial numbers. Given a small collection of serial numbers of tanks that were captured or destroyed, they attempted to estimate the total number of tanks produced, and with great effect.\n",
    "\n",
    "Formally, we a given a sample $X_1,\\ldots,X_n$ from a uniform distribution over $\\{1,\\ldots,M\\}$ and we want to estimate $M$. One obvious unbiased estimator is $$\\hat{M}_1 = \\frac{\\sum_{i=1}^n X_i}{n} \\cdot 2 - 1.$$\n",
    "However, there are other options, notably $$\\hat{M}_2 = \\max\\{X_1,\\ldots,X_n\\} \\cdot \\frac{n+1}{n}-1.$$\n",
    "\n",
    "In this problem, your goal is to:\n",
    " * Verify empirically that the second estimator is indeed unbiased (this will also be proved formally in the class).\n",
    " * Decide which of the two estimators is more efficient (has lower variance).\n",
    " \n",
    "**Note:** For the second estimator to be unbiased, the sampling has to be without replacement. This is also more realistic in the tank problem context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5b (Testing a sampler).** In this problem we will attempt to check whether the sampler we created in **Problem 2c** works correctly. To this end we will use a chi-squared goodness-of-fit test. This test works as follows:\n",
    " * Let $p_1,\\ldots,p_d$ be the date frequencies as in the text file, scaled down to sum up to 1.\n",
    " * Use the sampler to generate a sample of dates. Let $c_1,\\ldots,c_d$ be the observed counts, and let $f_i=Np_i$ be the expected counts, where $N$ is the sample size. \n",
    " * Compute the test statistic $$S = \\sum_{i=1}^d \\frac{\\left(c_i-f_i\\right)^2}{f_i}.$$\n",
    " * Our base assumption (the null hypothesis) $H_0$ is that our sampler works correctly. If $H_0$ is true AND if the expected count for each bucket is large enough, then $S$ has (approximately) a $\\chi^2$ distribution with $d-1$ degrees of freedom. \n",
    " * Look up how likely is getting an $S$ value as large as the one you obtained if it has that distribution, i.e. the $p$-value. To do this use **scipy.stats.chi2.cdf**, or even better, **scipy.stats.chi2.sf**. If this value turns out smaller than the assumed threshold, e.g. $0.05$, we reject $H_0$. Otherwise we do not (we support $H_0$), but this does not mean $H_0$ is proved!\n",
    " * We mentioned earlier that expected counts for the buckets need to be large enough. \"Large enough\" assumption here is used to guarantee that $c_i$ are distributed approximately normally. Typically one requires that all counts are at least $5$. This is not the case in our problem (unless we take a huge sample) because of the errors in the data. The typical approach is to glue several buckets into one but this does not help in our case. Instead, ignore the erroneous dates when computing $c_i$ and $f_i$ and run the test again (on the same sample!). Remember to use a different number of degrees of freedom. Compare the results. \n",
    " * Perform the same test using **scipy.stats.chisquare** and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5c (Chi-square independence test).** \n",
    "You are given the results of IPSOS exit polls for 2015 parliamentary elections in Poland in table **data**. Decide if we can assume that gender and voting preferences are independent. To this end:\n",
    " * Compute row totals $r_i$, column totals $c_j$, and overall total $N$.\n",
    " * If the variables are independent, we expect to see $f_{ij} = r_i c_j / N$ in $i$-th row $j$-th column.\n",
    " * Compute the test statistic as before, i.e. $$ S = \\sum_{ij} \\frac{\\left(f_{ij}-X_{ij}\\right)^2}{f_{ij}}.$$\n",
    " * Again test vs $\\chi^2$ CDF. However, if the variables are independent, we only have $(r-1)(c-1)$ degrees of freedom here (we only need to know the row and column totals).\n",
    " * The KORWiN party looks like an obvious outlier. Note, when we work with categorical variables we should not just remove a category -- it is better to aggregate them. Introduce an aggregated category by summing the votes for the parties with less than 5% total votes and repeat the experiment.\n",
    " \n",
    "**Note:** This kind of data is (to the best of our knowledge) not available online. It has been recreated based on\n",
    "online infographics and other tidbits of information available online. It is definitely not completely accurate, hopefully it is not very far off. Moreover, exit polls do not necessary reflect the actual distribution of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otrzymane S: 1473.4144793438045\n",
      "Otrzymane pvalue: 1.0\n",
      "Mam podstawy do odrzucenia hipotezy.\n",
      "\n",
      "Otrzymane S: 592.2303139981035\n",
      "Otrzymane pvalue: 1.0\n",
      "Mam podstawy do odrzucenia hipotezy.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "# Rows: women, men\n",
    "# Columns: PiS, PO, Kukiz, Nowoczesna, Lewica, PSL, Razem, KORWiN\n",
    "data = np.array([[ 17508, 11642,  3308,  3131,  2911,  2205,  1852, 1235],\n",
    " [ 17672,  9318,  4865,  3259,  3029,  2479,  1606, 3259]])\n",
    "\n",
    "# Poziom istotnośći poniżej, którego twierdzenie jest prawdobodobne.\n",
    "ALPHA = 0.05\n",
    "\n",
    "# Funkcja licząca S według wzoru podanego w zadaniu.\n",
    "def findS(r, c, N, dataToCheck):\n",
    "    S = 0\n",
    "\n",
    "    for i in range(len(dataToCheck)):\n",
    "        for j in range(len(dataToCheck[i])):\n",
    "            expected = r[i]*c[j]/N\n",
    "            S = S + ((dataToCheck[i][j] - expected)**2)/expected\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Funkcja zwraca stopień swobody.\n",
    "def deegreOfFreedom(c, r):\n",
    "    return (len(r)-1)*(len(c)-1)\n",
    "\n",
    "def checkHipotesis(dataToCheck):\n",
    "    # Sumuję wiersze.\n",
    "    r = [sum(x) for x in dataToCheck]\n",
    "\n",
    "    # Sumuję kolumny.\n",
    "    c = [sum(x) for x in zip(*dataToCheck)]\n",
    "\n",
    "    # Wyznaczam łączną liczbę głosów.\n",
    "    N = sum(sum(dataToCheck))\n",
    "    d = deegreOfFreedom(c, r)\n",
    "\n",
    "    S = findS(r, c, N, dataToCheck)\n",
    "    print(\"Otrzymane S: \" + str(S))\n",
    "\n",
    "    # Wyznaczam pvalue dla następującej hipotezy \"Płeć nie ma wpływu na wybór w głosowaniu.\"\n",
    "    pValue = scipy.stats.chi2.cdf(S, deegreOfFreedom(c, r))\n",
    "    print(\"Otrzymane pvalue: \" + str(pValue))\n",
    "\n",
    "    # Sprawdzam czy mogę przyjąć hipotezę.\n",
    "    if (pValue < ALPHA):\n",
    "        print(\"Mam podstawy do przyjęcia hipotezy.\")\n",
    "    else:\n",
    "        print(\"Mam podstawy do odrzucenia hipotezy.\")\n",
    "\n",
    "\n",
    "# Sprawdzam dla orginalnych wyników.\n",
    "checkHipotesis(data)\n",
    "\n",
    "# Sprawdzam co się dzieje bez partii KORWiN\n",
    "out_KORWiN_data = data[:, :-1]\n",
    "print()\n",
    "checkHipotesis(out_KORWiN_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla oryginalnych danych $S$ wychodzi bardzo duże, przez co $pvalue$ wyszło $1$. Usunięcie partii KORWiN, co prawda zmniejszyło $S$ prawie $3$ razy lecz dalej jest bardzo duże przez, co $pvalue$ ma dalej wartość $1$. Zatem wysoce prawdopodobne jest to, że płeć ma wpływ na głosowaną partię."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some students solving the problem above introduce various bugs, which cause them to get wrong conclusions (or right conclusions for wrong reason). Below is a simple code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def attempt():\n",
    "    # generate a similar dataset with 6 parties and two genders\n",
    "    # this time the null hypothesis is true, i.e., gender has no effect on voting preferences\n",
    "    data = np.zeros([2, 6])\n",
    "    for k in range(10000):\n",
    "      i = np.random.randint(2)\n",
    "      j = int(np.sqrt(np.random.randint(36)))\n",
    "      data[i][j] = data[i][j] + 1\n",
    "    # replace np.random.random() with the p-value returned by your implementation of chi-square independence test\n",
    "    # return chi_square_independence_test_pvalue(data)\n",
    "    return np.random.random()\n",
    "    \n",
    "pvalues = [attempt() for t in range(200)]\n",
    "print(pvalues[:20])\n",
    "# The values you get here should have (roughly) uniform distribution in [0,1].\n",
    "# Note: this means that, in 20 attempts, we are likely to find one to rejecting the null hypothesis,\n",
    "# even though the null hypothesis is true!\n",
    "plt.hist(pvalues)\n",
    "\n",
    "# to test whether the distribution of pvalue is indeed close to uniform distribution,\n",
    "# we can use the Kolmogorov-Smirnov test:\n",
    "scipy.stats.kstest(pvalues, \"uniform\")\n",
    "\n",
    "# Examine the pvalue returned by kstest.\n",
    "# Introduce a bug in your implementation (e.g. change the number of degrees of freedom) and see how the\n",
    "# pvalue returned by kstest changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5d (two-sample t-test).** We have two versions of a randomized optimization algorithm. In the table below you can find the objective function values for $n=20$ solutions found by each algorithm. Our goal is to decide if one of the algorithms is better than the other (i.e. finds solutions with higher values). We can proceed as follows:\n",
    " * Compute the means $M_1,M_2$. We need to decide if the difference $M_1-M_2$ is significant.\n",
    " * We estimate the standard deviation of $M_1-M_2$. Show that this is equal to $\\sqrt{(\\sigma_1^2+\\sigma_2^2)/n}$, where $\\sigma_i$ is the standard deviation of the value found by the algorithm $i$. We can estimate this by $D=\\sqrt{(\\hat{\\sigma_1^2}+\\hat{\\sigma_2^2})/n}$ where $\\hat{\\sigma_i^2}$ is an unbiased estimate of the variance of $i$-th algorithm.\n",
    " * Now compute $\\frac{M_1-M_2}{D}$. If the real values of $\\sigma_i$ are equal, i.e. $\\sigma_1=\\sigma_2$, then it can be shown that this has a $t$-distribution with $2n-2$ degrees of freedom. \n",
    " * Use **scipy.stats.t.cdf** to compute the p-value and decide if we can reject the null hyphotesis of the means being equal. Note that in our case the probability of getting a difference \"at least as large as the one we got\" has to include deviations in both directions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.array([ [98.67573, 100.77983, 101.35478,  98.50285,  99.14984, \n",
    "      100.64105,  98.37170, 100.24933, 99.54994, 100.79508, \n",
    "      101.14978, 101.32538, 100.44573, 97.60020, 97.78547,  \n",
    "      98.88703,  99.06794, 100.65313, 100.30297, 100.26311],\n",
    "       [99.96234, 99.94943, 99.58779, 100.52263, 101.58972,\n",
    "        101.78411, 100.09874, 100.99211, 101.93214, 100.61749,\n",
    "        100.46854,  99.19030, 101.28412, 100.70897, 99.83988,\n",
    "        100.24668,  99.38278,  99.82720,  97.55918, 100.63128]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
